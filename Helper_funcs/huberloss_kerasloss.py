# -*- coding: utf-8 -*-
"""HuberLoss_KerasLoss.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vi8mx5Tm7Otsdo68Qh_1Xyt6Sm4icgpL
"""

"""
An example of a custom loss function for DL models. This framework can be integrated into other workflows as needed.

This specific example demonstrates how to use Keras classes to build your own.

Boston housing data will be used as an example case for regression.

Huber loss is a robust loss function for regression tasks that is less sensitive to outliers. 

It should be noted that this is an example workflow for teaching purposes and exploratory analyses. More detailed and integrated 
pipelines are typically created for MLOps.

"""

#%%---------------------------------------
#Import libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


from sklearn.datasets import load_boston
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit

import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense,Flatten,Dropout

from sklearn.metrics import classification_report,confusion_matrix,r2_score,mean_squared_error,mean_absolute_error
#%%---------------------------------------
#Helper functions

def reg_model_eval(history,epochs):

  """
  A regression model evaluation using history and epoch parameters

  #Arguments 
    history (keras obj) - keras history object
    epochs  (keras obj) - keras epcohs object
  #Returns
    training an dvalidation plots for regression analysis
  """
  #NOTE due to train and validation set calculation specifics epochs need to be shifted by 0.5 according to some ML practitioners
  #Assess this based on your plots if that is necessary
  plt.plot(epochs,history['mean_squared_error'],'b-',label="Train MSE")
  plt.plot(epochs,history['val_mean_squared_error'],'r-',label="Validation MSE")
  plt.legend()
  plt.show()

  plt.plot(epochs,history['mean_absolute_error'],'b-',label="Train MAE")
  plt.plot(epochs,history['val_mean_absolute_error'],'r-',label="Validation MAE")
  plt.legend()
  plt.show()
  return


from sklearn.model_selection import StratifiedShuffleSplit
from pandas.api.types import is_numeric_dtype
import pandas as pd


def stratified_split(data:pd.DataFrame,target: pd.DataFrame):

    """
    Function takes data as a dataframe and a variable as a string value to perform a stratified split.

    Test set size is 20%

    data, pd.DataFrame - data to be split
    target, array - target data which is used for splitting
    """

    data=data.copy()

    
    #If categorical value is already in place use that to split the data

    split=StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=0)

    #Target values are used for the splitting
    for train_idx,test_idx in split.split(data,target):


      X_train=data.iloc[train_idx,:]
      y_train=target[train_idx]

      X_test=data.iloc[test_idx,:]
      y_test=target[test_idx]

    return X_train, X_test,y_train,y_test

#%%------------------------------------------------------------
#Load data

data_boston=load_boston()

data=pd.DataFrame(data=data_boston.data,columns=data_boston.feature_names)
target=data_boston.target

#Check the data (high level overview)
#NOTE you should build a separate custom set of functions for data integrity checks and EDA

plt.hist(target,bins=100)
plt.title("Target value distribution")
plt.show()

df=data.copy()
df['target']=target
print(df.describe(),"\n")

print(data.head(),"\n")

df.plot.box(figsize=(20,20))
plt.show()

sns.pairplot(df,diag_kind='hist')
plt.show()
#NOTE outlier, multicollinearity and other effects should be assessed and data should be cleaned before further processing
#There are additional considerations (e.g., cutoff values introduced in this data, etc.)
#For the example purposes, this will not be done

#We will include a qualitative feature for the data by splitting target values into low (0), medium (1), and high(2)
#The numbering will be used to avoid additional label binarisation.
#NOTE the indexing has to start with 0 for sparse_categorical_crossentropy
#NOTE splitting can affect how the data is trained, especially if it leads to categories that have very samples.
#For demonstration purposes we are selecting a simple model split.
#Check how data distributes when we set specific categories. That is low(0), medium(1) and high(2) values.

#%%------------------------------------------------------------
#For demonstration purposes we are selecting a simple model split.
target_class=pd.qcut(target,q=[0.0,0.25,0.75,1.0],labels=[0,1,2])

df['target_class']=target_class

#Use pairplots to quickly evaluate any dependencies or cutoffs in the data

sns.pairplot(df,diag_kind='kde',hue='target_class')
plt.show()

#Check for value correlation
sns.heatmap(df.corr(method='spearman'),annot=True)
plt.show()

print(df.describe())

targets_df=pd.DataFrame(target,columns=['target'])
targets_df['target_class']=np.array(target_class,dtype='float32')
targets_df.head()

targets_df[targets_df['target_class'].isna()]

#Prepare data for modelling

#%%------------------------------------------------------------
#Split data
#Consider a stratified split using synthetic categories to better optimise for regression
split_data=data.copy()
split_data['target']=target
X_train,X_test,y_train,y_test=stratified_split(split_data,target_class)

#Evaluate how the data was split
sns.countplot(y_train)
plt.title("Train categories")
plt.show()

plt.hist(X_train['target'],bins=100)
plt.title("Train target distribution")
plt.show()

sns.countplot(y_test)
plt.title("Test categories")
plt.show()

plt.hist(X_test['target'],bins=100)
plt.title("Test target distribution")
plt.show()


#Extract targets for regression and scale data

X_train,y_train=X_train.drop(columns=['target']),X_train['target']
X_test,y_test=X_test.drop(columns=['target']),X_test['target']

#Scale data
sc=StandardScaler()
X_train=sc.fit_transform(X_train)
X_test=sc.transform(X_test)


#%%------------------------------------------------------------
#Set params

EPOCHS=500
BATCH=50
VAL_SPLIT=0.2
NUM_CLASSES=3
INPUT_SHAPE=X_train.shape[1:]
callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=10, min_delta=1)


#Create a simple regression model

def reg_model(units=10,input_shape=INPUT_SHAPE, custom_loss=None):
  """
  A simple model builder to establish a baseline regression 
  #NOTE several baseline models can be used to better assess target model performance
  
  #Arguments
  units, int - dense layer units
  input_shape, tuple(int) - input shape (None, Features)
  custom_loss, function - a custom loss function, default value is None
  #Returns
  model (keras obj)

  #NOTE additional checks should be added for the custom function when integrating into pipelines
  """
  model=Sequential()

  model.add(Dense(units,activation='relu',input_shape=input_shape))


  model.add(Dense(1))

  model.summary()
  if custom_loss is None:
    model.compile(loss="mean_squared_error",optimizer='adam',metrics=['mean_squared_error','mean_absolute_error'])
  else:
    model.compile(loss=custom_loss,optimizer='adam',metrics=['mean_squared_error','mean_absolute_error'])
  return model


#%%------------------------------------------------------------
#Create a baseline classification model
model=reg_model()

history=model.fit(X_train,y_train,epochs=EPOCHS,batch_size=BATCH,validation_split=VAL_SPLIT,callbacks=[callbacks])


#Evaluate the model
epochs=history.epoch
history=history.history

print("Best epoch",epochs[np.argmin(history['val_loss'])])

reg_model_eval(history,epochs)


#%%------------------------------------------------------------

#Evaluate functional model
#Define Huber loss
#NOTE this definition allows you to set and save thresholds when loading the model
class HuberLoss(keras.losses.Loss):
    """
  
    threshold, float - threshold for the loss optimisation, default value is 0.5

    returns a call for a function that can be loaded into keras object

    #Arguments:
    y_true, array(float/int) - true values
    y_pred, array(float/int) - predicted values
    threshold, float - setting a weight value for different losses
    #Retunrs:
    tf object for the smallest error value
    """

    def __init__(self, threshold=0.5,**kwargs):
        #Initiate keras Loss class
        super().__init__()
        self.threshold=threshold

    def call(self,y_true,y_pred):
        #Huber loss is calculated via call function
        error=y_true-y_pred

        
        thr=tf.abs(error)<self.threshold

        squared_loss=tf.square(error)/2
        linear_loss=tf.abs(error)*self.threshold-self.threshold**2/2

        return tf.where(thr,squared_loss,linear_loss)

    def get_config(self):
        #Configuration allows to save model threshold 
        config=super.get_config()
        return {**config, 'threshold':self.threshold}



#%%------------------------------------------------------------


model_customloss=reg_model(custom_loss=HuberLoss(25.0))

history=model_customloss.fit(X_train,y_train,epochs=EPOCHS,batch_size=BATCH,validation_split=VAL_SPLIT,callbacks=[callbacks])


#Evaluate the model
epochs=history.epoch
history=history.history

print("Best epoch for the custom loss",epochs[np.argmin(history['val_loss'])])

reg_model_eval(history,epochs)

#%%------------------------------------------------------------

#Evaluate models

simple_model_results=model.evaluate(X_test, y_test)
y_pred_simple=model.predict(X_test)
print("R2 for the simple model evaluation\n",r2_score(y_test,y_pred_simple),"\n")
print("Simple model results \n",simple_model_results,"]\n")

model_customloss_results=model_customloss.evaluate(X_test, y_test)
y_pred_custom=model_customloss.predict(X_test)
print("R2 for the custom model evaluation\n",r2_score(y_test,y_pred_custom),"\n")
print("custom model results \n",model_customloss_results,"\n")

#%%------------------------------------------------------------