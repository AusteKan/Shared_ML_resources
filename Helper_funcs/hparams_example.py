# -*- coding: utf-8 -*-
"""HParams_example.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O7vDkr8RnJJD9ECZe4DuAHCar6Lflu9r

HParams allows a simple implementatiomn of hyperparameter tuning uisng tensorboard.

This example workflow explores some of the key features that could be adapted to individual projects.

This is an example; thus, typical MLOps are more complex involving additional functionalities.
"""

!pip install tensorboard

#Import libraries

import numpy as numpy
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf
import tensorboard as tb
from tensorboard.plugins.hparams import api as hp
from tensorflow import keras

from keras.datasets import fashion_mnist

# Commented out IPython magic to ensure Python compatibility.
#Setup tensorboad
# %load_ext tensorboard
!rm -rf ./logs/

#Load data
(X_train,y_train),(X_test,y_test)=fashion_mnist.load_data()

#Prepare data
X_train=X_train.astype('float32')/255.0
X_test=X_test.astype('float32')/255.0

print("Train data",X_train.shape)
print("Test data",X_test.shape)

#Define hyperparameters to test in the model

#Number of units in a hidden layer
HP_units=hp.HParam("n_units",hp.Discrete([10,64,128,256]))
#Dropout rate for a dropout layer
HP_dropout=hp.HParam('dropout',hp.RealInterval(0.2,0.3))
#Optimizer type
HP_opt=hp.HParam('optimizer',hp.Discrete(['adam','sgd','rmsprop']))
#Learning rate for an optimizer
HP_lr=hp.HParam('learning_rate',hp.Discrete([10e-3,10e-4,10e-5]))

#Define metrics
METRICS='accuracy'

#Log experiment configurations
with tf.summary.create_file_writer('/logs/HParam_tuning').as_default():
  hp.hparams_config(hparams=[HP_units,HP_dropout,HP_opt,HP_lr],metrics=[hp.Metric(METRICS,display_name='accuracy')])

#Define model

def model(hp):

  model=tf.keras.models.Sequential()
  model.add(tf.keras.layers.Flatten())
  model.add(tf.keras.layers.Dense(hp[HP_units]*2,activation='relu'))
  model.add(tf.keras.layers.Dropout(hp[HP_dropout]))
  model.add(tf.keras.layers.Dense(hp[HP_units],activation='relu'))
  model.add(tf.keras.layers.Dropout(hp[HP_dropout]))
  model.add(tf.keras.layers.Dense(hp[HP_units]//2,activation='relu'))
  model.add(tf.keras.layers.Dropout(hp[HP_dropout]))

  model.add(tf.keras.layers.Dense(10,activation='softmax'))

  opt=hp[HP_opt]
  if opt=='adam':
    optimizer=tf.keras.optimizers.Adam(learning_rate=hp[HP_lr])
  elif opt=='sgd':
    optimizer=tf.keras.optimizers.SGD(learning_rate=hp[HP_lr])    
  elif opt=='rmsprop':
    optimizer=tf.keras.optimizers.SGD(learning_rate=hp[HP_lr])   
  model.compile(optimizer=optimizer,loss='sparse_categorical_crossentropy',metrics=['accuracy'])

  model.fit(X_train,y_train,epochs=20,batch_size=512)
  loss,acc=model.evaluate(X_test,y_test)
  return acc

#Record values for each trial
def log_run(run_dir,hparams):
  with tf.summary.create_file_writer(run_dir).as_default():
    hp.hparams(hparams) 
    acc=model(hparams)
    tf.summary.scalar(METRICS,acc,step=1)

#Try different configurations using a simple grid search

session=0

for n_units in  HP_units.domain.values:
  for n_drp in  (HP_dropout.domain.min_value,HP_dropout.domain.max_value):
    for opt in  HP_opt.domain.values:
      for lr in  HP_lr.domain.values:
        hparams={
            HP_units:n_units,
            HP_dropout:n_drp,
            HP_opt:opt,
            HP_lr:lr
        }

        run_name="run-%d"%session
        print('---Trial: %s'%run_name)
        print({h.name:hparams[h] for h in hparams})

        log_run('logs/HParam_tuning/'+run_name,hparams)

        session+=1

# Commented out IPython magic to ensure Python compatibility.
#Visualise the results
# %tensorboard --logdir logs/HParam_tuning