# -*- coding: utf-8 -*-
"""GNN_classification_keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-QpupRUCs_vZSmJLsMW0-bmO0DKVvzIe

This example focuses on a simple implementation of a Graph Neural Network (GNN). Paper and their citation dataset is used for modelling.
The model predicts the subject of a paper given its words and citations network.

We first perform an EDA, then we build a couple of baseline models. Finally, we implement a graph convolutional network (GCN) to predict paper subject.
"""

!pip install -q tensorflow-gnn

!pip install 'scipy>=1.8'

!pip install 'networkx<2.7' #NOTE networkx might have dependency issues with scipy; thus reinstall is advised.

#Import libraries
import os

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

import networkx as nx

import tensorflow as tf
import tensorflow_gnn as tfgnn

from sklearn.metrics import classification_report,confusion_matrix

#Define a helper function

def classification_summary(y_test,y_pred,labels=None):
  """
  Function takes test labels and predicted labels to return a classification report and a confusion matrix heatmap

  #Args:
    y_test, array(int/float)-test labels
    y_pred, array(int/float)-predicted labels
    labels, array/list(str)- names for different predicted classes
  #Returns:
    classification report and a confusion matrix heatmap
  """

  print(classification_report(y_test,y_pred))

  sns.heatmap(confusion_matrix(y_test,y_pred),annot=True, cmap='Blues',fmt='g',xticklabels=labels,yticklabels=labels)
  plt.xlabel('Predicted values')
  plt.ylabel('True values')
  plt.show()

  return

#Import data

file=tf.keras.utils.get_file(fname="cora.tgz",
    origin="https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz",
    extract=True)

path=os.path.join(os.path.dirname(file),'cora')

#Check data
! ls -al {path}

#Process data
citations=pd.read_csv(os.path.join(path, "cora.cites"),
    sep="\t",
    header=None,
    names=["target", "source"])

columns=["ID"]+[f"term_{i}" for i in range(1433)]+["subject"]
papers=pd.read_csv(os.path.join(path, "cora.content"),
    sep="\t",
    header=None,
    names=columns)

#Source column describes paper IDS and the target column describes a cited paper ID
print(citations.shape)
print(citations.head())
print(citations.sample(frac=1).head())

print(papers.shape)
print(papers.head())

#Explore data

print(papers.subject.value_counts())

sns.countplot(x=papers.subject)
plt.xticks(rotation=90)
plt.show()

#Convert IDs to indexes

class_idxs={name:id for id,name in enumerate(sorted(papers['subject'].unique()))}
paper_idxs={name:id for id,name in enumerate(sorted(papers['ID'].unique()))}


citations['target']=citations['target'].apply(lambda name: paper_idxs[name])
citations['source']=citations['source'].apply(lambda name: paper_idxs[name])

papers['ID']=papers['ID'].apply(lambda name: paper_idxs[name])
papers['subject']=papers['subject'].apply(lambda name: class_idxs[name])

#Explore graph
#NOTE we are sampling a set of nodes to make the mapping more efficient

graph_sample=nx.from_pandas_edgelist(citations.sample(n=200))
graph_vals=list(papers[papers["ID"].isin(list(graph_sample.nodes))]['subject'])
nx.draw_spring(graph_sample,node_color=graph_vals,node_size=10)

papers.head()

#Split data

from sklearn.model_selection import train_test_split

#We need to preprocess data to extract classes and features
X=papers.iloc[:,:-1]
y=papers.iloc[:,-1]

feature_names=X.columns

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)


X_train_baseline=X_train.iloc[:,1:].to_numpy()
y_train=y_train.to_numpy()
X_test_baseline=X_test.iloc[:,1:].to_numpy()
y_test=y_test.to_numpy()

#Define parameters
BATCH=128
EPOCHS=250
INPUT=X_train_baseline.shape[1]
CLASS_NUM=len(y.unique())
hidden_units=[32,32,16]

es=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=50,restore_best_weights=True)

#Create a basline model
baseline_model=tf.keras.Sequential()

baseline_model.add(tf.keras.layers.Input(shape=(INPUT,)))
baseline_model.add(tf.keras.layers.BatchNormalization())
baseline_model.add(tf.keras.layers.Dense(128,activation='gelu'))
baseline_model.add(tf.keras.layers.Dropout(0.2))
baseline_model.add(tf.keras.layers.BatchNormalization())
baseline_model.add(tf.keras.layers.Dense(64,activation='gelu'))
baseline_model.add(tf.keras.layers.Dropout(0.2))
baseline_model.add(tf.keras.layers.BatchNormalization())
baseline_model.add(tf.keras.layers.Dense(32,activation='gelu'))
baseline_model.add(tf.keras.layers.Dropout(0.2))
baseline_model.add(tf.keras.layers.BatchNormalization())
baseline_model.add(tf.keras.layers.Dense(16,activation='gelu'))
baseline_model.add(tf.keras.layers.Dropout(0.2))

baseline_model.add(tf.keras.layers.Dense(CLASS_NUM,activation='softmax'))

baseline_model.summary()

baseline_model.compile(optimizer=tf.keras.optimizers.Adam(0.001),loss=tf.keras.losses.SparseCategoricalCrossentropy(),metrics=['accuracy'])

baseline_history=baseline_model.fit(X_train_baseline,y_train,epochs=EPOCHS, batch_size=BATCH,validation_split=0.1,callbacks=[es])

baseline_results=baseline_history.history
epoch=len(baseline_history.epoch)
baseline_results.keys()

fig,ax=plt.subplots(1,2, figsize=(10,5))
ax.flatten()
ax[0].plot(np.arange(1,epoch+1),baseline_results['loss'],'b-',label='Training loss')
ax[0].plot(np.arange(1,epoch+1),baseline_results['val_loss'],'r-',label='Val loss')
ax[0].legend()
ax[1].plot(np.arange(1,epoch+1),baseline_results['accuracy'],'b-',label='Training accuracy')
ax[1].plot(np.arange(1,epoch+1),baseline_results['val_accuracy'],'r-',label='Val accuracy')
ax[1].legend()
plt.show()

#Evaluate baseline model

baseline_test_loss,baseline_test_acc=baseline_model.evaluate(X_test_baseline,y_test)
print(f"Baseline model test accuracy {baseline_test_acc} and loss {baseline_test_loss}")

#Predict values
y_pred=baseline_model.predict(X_test_baseline)
y_pred_vals=np.argmax(y_pred,axis=1)
classification_summary(y_test,y_pred_vals,labels=class_idxs.keys())

#Create a baseline model to be more similar to GNN

#Include skipped nodes

def create_ffn(hidden_units, dropout_rate, name=None):
    #Implement Feedforward Network module
    fnn_layers = []

    for units in hidden_units:
        fnn_layers.append(tf.keras.layers.BatchNormalization())
        fnn_layers.append(tf.keras.layers.Dense(units, activation=tf.nn.gelu))
        fnn_layers.append(tf.keras.layers.Dropout(dropout_rate))
        
    return tf.keras.Sequential(fnn_layers, name=name)

    

def create_baseline_model(hidden_units, num_classes, dropout_rate=0.2):

    inputs = tf.keras.layers.Input(shape=(INPUT,), name="input_features")
    x = create_ffn(hidden_units, dropout_rate, name=f"ffn_block1")(inputs)

    for block_idx in range(1,5):
        # Create an FFN block
        x1 = create_ffn(hidden_units, dropout_rate, name=f"ffn_block{block_idx + 1}")(x)
        # Add skip connection
        x = tf.keras.layers.Add(name=f"skip_connection{block_idx + 1}")([x, x1])

    # Get model probabilities
    output = tf.keras.layers.Dense(num_classes, activation='softmax',name="prob")(x)
    # Create the model
    return tf.keras.Model(inputs=inputs, outputs=output, name="baseline")

baseline_model2=create_baseline_model(hidden_units, CLASS_NUM, dropout_rate=0.25)
baseline_model2.summary()
baseline_model2.compile(optimizer=tf.keras.optimizers.Adam(0.001),loss=tf.keras.losses.SparseCategoricalCrossentropy(),metrics=['accuracy'])

baseline_history2=baseline_model2.fit(X_train_baseline,y_train,epochs=EPOCHS, batch_size=BATCH,validation_split=0.1,callbacks=[es])

baseline_results=baseline_history2.history
epoch=len(baseline_history2.epoch)
baseline_results.keys()

fig,ax=plt.subplots(1,2, figsize=(10,5))
ax.flatten()
ax[0].plot(np.arange(1,epoch+1),baseline_results['loss'],'b-',label='Training loss')
ax[0].plot(np.arange(1,epoch+1),baseline_results['val_loss'],'r-',label='Val loss')
ax[0].legend()
ax[1].plot(np.arange(1,epoch+1),baseline_results['accuracy'],'b-',label='Training accuracy')
ax[1].plot(np.arange(1,epoch+1),baseline_results['val_accuracy'],'r-',label='Val accuracy')
ax[1].legend()
plt.show()

#Evaluate baseline model

baseline_test_loss,baseline_test_acc=baseline_model2.evaluate(X_test_baseline,y_test)
print(f"Baseline model test accuracy {baseline_test_acc} and loss {baseline_test_loss}")

#Predict values
y_pred=baseline_model2.predict(X_test_baseline)
y_pred_vals=np.argmax(y_pred,axis=1)
classification_summary(y_test,y_pred_vals,labels=class_idxs.keys())

#Build GNN model

#This example demonstrates a simple approach to prepare graph data where you have
# a single graph that fits entirely in memory.


"""
The graph data is represented by the graph_info tuple:

    node_features, array[num_nodes, num_features] - an array that includes the node features. 
    The nodes are the papers, and the node_features are  binary 
    vectors for each paper.

    edges, array[num_edges, num_edges] -  a sparse adjacency matrix of the links between the nodes. 
    The links are the citations between the papers.

    edge_weights (optional), array[num_edges] - an array that includes the edge weights, 
    which quantify the relationships between nodes in the graph
"""


# Create an edges array (sparse adjacency matrix)  [2, num_edges]
edges = citations[["source", "target"]].to_numpy().T
# Create an edge weights array of ones
edge_weights = tf.ones(shape=edges.shape[1])
# Create a node features array [num_nodes, num_features]
node_features = tf.cast(papers.sort_values("ID")[feature_names].to_numpy(), dtype=tf.dtypes.float32)
# Create graph info tuple 
graph_info = (node_features, edges, edge_weights)

print("Edges shape:", edges.shape)
print("Nodes shape:", node_features.shape)

"""###Implementing a graph convolution layer

We are going to use keras layers for our GCN. Key steps involve preparation to produce a message, then we aggregate neigbours features using edge weights and finally we update the node information.

### NOTE 
GRU requires sequence stacking while other aggregation types use concatenation.

"""

class GraphConvLayer(tf.keras.layers.Layer):
    def __init__(self,hidden_units,
        dropout_rate=0.2, aggregation_type="mean",
        combination_type="concat",normalize=False,
        *args,**kwargs):
        super().__init__(*args, **kwargs)

        self.aggregation_type = aggregation_type
        self.combination_type = combination_type
        self.normalize = normalize

        #Prepare input
        self.ffn_prepare = create_ffn(hidden_units, dropout_rate)

        #Define update function
        if self.combination_type == "gated":

            self.update_fn = tf.keras.layers.GRU(
                units=hidden_units,
                activation="tanh",
                recurrent_activation="sigmoid",
                dropout=dropout_rate,
                return_state=True,
                recurrent_dropout=dropout_rate,
            )
        else:
            self.update_fn = create_ffn(hidden_units, dropout_rate)

    def prepare(self, node_repesentations, weights=None):
        # node_repesentations shape is [num_edges, embedding_dim]
        messages = self.ffn_prepare(node_repesentations)
        if weights is not None:
            messages = messages * tf.expand_dims(weights, -1)
        return messages

    def aggregate(self, node_indices, neighbour_messages, node_repesentations):
        # node_indices shape is [num_edges]
        # neighbour_messages shape: [num_edges, representation_dim]
        # node_repesentations shape is [num_nodes, representation_dim]


        num_nodes = node_repesentations.shape[0]

        if self.aggregation_type == "sum":
            aggregated_message = tf.math.unsorted_segment_sum(
                neighbour_messages, node_indices, num_segments=num_nodes)
        elif self.aggregation_type == "mean":
            aggregated_message = tf.math.unsorted_segment_mean(
                neighbour_messages, node_indices, num_segments=num_nodes)
        elif self.aggregation_type == "max":
            aggregated_message = tf.math.unsorted_segment_max(
                neighbour_messages, node_indices, num_segments=num_nodes)
        else:
            raise ValueError(f"Invalid aggregation type: {self.aggregation_type}.")

        return aggregated_message

    def update(self, node_repesentations, aggregated_messages):

        # node_repesentations shape is [num_nodes, representation_dim]
        # aggregated_messages shape is [num_nodes, representation_dim]


        if self.combination_type == "gru":
            # Create a sequence of two elements for the GRU layer.
            h = tf.stack([node_repesentations, aggregated_messages], axis=1)
        elif self.combination_type == "concat":
            # Concatenate the node_repesentations and aggregated_messages.
            h = tf.concat([node_repesentations, aggregated_messages], axis=1)
        elif self.combination_type == "add":
            # Add node_repesentations and aggregated_messages.
            h = node_repesentations + aggregated_messages
        else:
            raise ValueError(f"Invalid combination type: {self.combination_type}.")

        # Process node embedings
        node_embeddings = self.update_fn(h)
        if self.combination_type == "gru":
            node_embeddings = tf.unstack(node_embeddings, axis=1)[-1]

        if self.normalize:
            node_embeddings = tf.nn.l2_normalize(node_embeddings, axis=-1)
        return node_embeddings

    def call(self, inputs):
        """
        GCN generates node embedings

        Inputs: a tuple of node_repesentations, edges, edge_weights

        Returns: node_embeddings of shape [num_nodes, representation_dim]
        """

        node_repesentations, edges, edge_weights = inputs

        # Get node_indices (source) and neighbour_indices (target)
        node_indices, neighbour_indices = edges[0], edges[1]

        # neighbour_repesentations shape is [num_edges, representation_dim]
        neighbour_repesentations = tf.gather(node_repesentations, neighbour_indices)

        # Prepare the messages of the neighbours
        neighbour_messages = self.prepare(neighbour_repesentations, edge_weights)

        # Aggregate the neighbour messages
        aggregated_messages = self.aggregate(node_indices, neighbour_messages, node_repesentations)

        # Update the node embedding with the neighbour messages
        return self.update(node_repesentations, aggregated_messages)

"""### Implement GCN node classifier

The GNN classification model follows the Design Space for Graph Neural Networks approach.

###NOTE
Each additional convolutional layer captures information from a further level of neighbours. This can lead to oversmoothing if you add too many layers.

"""

class GNNNodeClassifier(tf.keras.Model):
    def __init__(self,
        graph_info,
        num_classes,
        hidden_units,
        aggregation_type="sum",
        combination_type="concat",
        dropout_rate=0.2,
        normalize=True,
        *args,
        **kwargs):
        super().__init__(*args, **kwargs)

        node_features, edges, edge_weights = graph_info
        self.node_features = node_features
        self.edges = edges
        self.edge_weights = edge_weights


        # Set edge_weights to ones if not provided
        if self.edge_weights is None:
            self.edge_weights = tf.ones(shape=edges.shape[1])

        # Scale edge_weights to sum to 1
        self.edge_weights = self.edge_weights / tf.math.reduce_sum(self.edge_weights)

        # Create a process layer
        self.preprocess = create_ffn(hidden_units, dropout_rate, name="preprocess")

        # Create the first GraphConv layer
        self.conv1 = GraphConvLayer(
            hidden_units,
            dropout_rate,
            aggregation_type,
            combination_type,
            normalize,
            name="GCN1")
        
        
        # Create the second GraphConv layer
        self.conv2 = GraphConvLayer(
            hidden_units,
            dropout_rate,
            aggregation_type,
            combination_type,
            normalize,
            name="GCN2")
        
        # Create a postprocess layer
        self.postprocess = create_ffn(hidden_units, dropout_rate, name="postprocess")
        # Create a prob layer
        self.compute_prob = tf.keras.layers.Dense(units=num_classes, activation='softmax')

    def call(self, input_node_indices):
        # Preprocess the node_features to produce node representations
        x = self.preprocess(self.node_features)
        # Apply the first graph conv layer
        x1 = self.conv1((x, self.edges, self.edge_weights))
        # Add skip connection
        x = x1 + x
        # Apply the second graph conv layer
        x2 = self.conv2((x, self.edges, self.edge_weights))
        # Add skip connection
        x = x2 + x
        # Postprocess node embedding
        x = self.postprocess(x)
        # Fetch node embeddings for the input node_indices
        node_embeddings = tf.gather(x, input_node_indices)
        # Compute probabilities for node embedings

        return self.compute_prob(node_embeddings)

gnn_model = GNNNodeClassifier(
    graph_info=graph_info,
    num_classes=CLASS_NUM,
    hidden_units=[32,32,16],
    dropout_rate=0.2,
    name="GCN_model")

print("GCN output shape:", gnn_model([1, 10, 100]))

gnn_model.summary()

gnn_model.compile(optimizer=tf.keras.optimizers.Adam(0.001),loss=tf.keras.losses.SparseCategoricalCrossentropy(),metrics=['accuracy'])

history = gnn_model.fit(X_train.ID.to_numpy(), y_train,epochs=EPOCHS, batch_size=BATCH,validation_split=0.1,callbacks=[es])

results=history.history
epoch=len(history.epoch)
results.keys()
fig,ax=plt.subplots(1,2, figsize=(10,5))
ax.flatten()
ax[0].plot(np.arange(1,epoch+1),results['loss'],'b-',label='Training loss')
ax[0].plot(np.arange(1,epoch+1),results['val_loss'],'r-',label='Val loss')
ax[0].legend()
ax[1].plot(np.arange(1,epoch+1),results['accuracy'],'b-',label='Training accuracy')
ax[1].plot(np.arange(1,epoch+1),results['val_accuracy'],'r-',label='Val accuracy')
ax[1].legend()
plt.show()

#Evaluate baseline model

test_loss,test_acc=gnn_model.evaluate(X_test.ID.to_numpy(),y_test)
print(f"Baseline model test accuracy {test_acc} and loss {test_loss}")

#Predict values
y_pred=gnn_model.predict(X_test.ID.to_numpy())
y_pred_vals=np.argmax(y_pred,axis=1)
classification_summary(y_test,y_pred_vals,labels=class_idxs.keys())